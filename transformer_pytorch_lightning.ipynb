{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UixW-FF8Qql"
      },
      "source": [
        "# ! pip install pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mop70UkX6z3Q"
      },
      "source": [
        "# Standard libraries\n",
        "import math\n",
        "import os\n",
        "import urllib.request\n",
        "from functools import partial\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "# Plotting\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# PyTorch Lightning\n",
        "import pytorch_lightning as pl\n",
        "import seaborn as sns\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from IPython.display import set_matplotlib_formats\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di_OiM468zEH"
      },
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits.masked_fill_(mask == 0, -9e15)\n",
        "    attention = F.softmax(attn_logits)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evfabxYBG0hs"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def _reset_parameters_(self):\n",
        "        nn.init.xavier_uniform_(self.qkv_proj)\n",
        "        nn.init.xavier_uniform_(self.o_proj)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        batch, seq_length, embed_dim = x.shape()\n",
        "        qkv = self.qkv_proj(x)\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.permute(0, 2, 1, 3)\n",
        "        values.reshape(batch_size, seq_length, embed_dim)\n",
        "        o = self.o_proj(values)\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GWvD2nYHvjB"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.multihead_attention = MultiheadAttention(input_dim, input_dim, num_heads)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(input_dim, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, input_dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attention_out = self.multihead_attention(x, mask)\n",
        "        x = x + self.dropout(attention_out)\n",
        "        x = self.norm1(x)\n",
        "        linear_out = self.feed_forward(x)\n",
        "        x = x + self.dropout(linear_out)\n",
        "        x = self.norm2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--QLV3bnOlXP"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, **block_args):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=mask)\n",
        "        return x\n",
        "\n",
        "    def get_attention_maps(self, x, mask=None):\n",
        "        attention_maps = []\n",
        "        for layer in self.layers:\n",
        "            _, attn_map = layer.multihead_attention(x, mask=mask, return_attention=True)\n",
        "            attention_maps.append(attn_map)\n",
        "            x = layer(x, mask=mask)\n",
        "        return attention_maps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6jUOF-XTi3o"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(-1 * torch.arange(0, max_len, 2).float() * math.log(1000) / d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.shape(1)]\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, warmup, max_itrs):\n",
        "        super().__init__(optimizer)\n",
        "        self.warmup = warmup\n",
        "        self.max_itrs = max_itrs\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
        "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
        "\n",
        "    def get_lr_factor(self, epoch):\n",
        "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_itrs))\n",
        "        if epoch <= self.warmup:\n",
        "            lr_factor *= epoch * 1.0 / self.warmup\n",
        "        return lr_factor"
      ],
      "metadata": {
        "id": "mIkL_WmSJb-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerPredictor(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.__create_model()\n",
        "\n",
        "    def __create_model(self):\n",
        "        self.input_nn = nn.Sequential(\n",
        "            nn.Linear(self.hparams.input_dim, self.hparams.model_dim),\n",
        "            nn.Dropout(self.hparams.input_dropout)\n",
        "        )\n",
        "        self.position_encoding = PositionalEncoding(d_model=self.hparams.input_dim)\n",
        "        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers, \n",
        "                                              input_dim=self.hparams.model_dim, \n",
        "                                              num_heads=self.hparams.num_heads, \n",
        "                                              dim_feedforward=self.hparams.model_dim, \n",
        "                                              dropout=self.hparams.dropout)\n",
        "        self.output_nn = nn.Sequential(\n",
        "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
        "            nn.LayerNorm(self.hparams.model_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(self.hparams.dropout),\n",
        "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
        "        x = self.input_nn(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.position_encoding(x)\n",
        "        x = self.tranformer(x, mask)\n",
        "        x = self.output_nn(x)\n",
        "        return x\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "        self.lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=self.hparams.warmup, max_itrs=self.hparams.max_itrs)\n",
        "        return optimizer\n",
        "\n",
        "    def optimizer_step(self, *args, **kwargs):\n",
        "        super().optimizer_step(*args, **kwargs)\n",
        "        self.lr_scheduler.step()"
      ],
      "metadata": {
        "id": "TgcWxB8VR2T1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}